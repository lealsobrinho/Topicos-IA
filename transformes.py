# -*- coding: utf-8 -*-
"""Transformes-1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SFfTsPbvV_kviWa0yMz-ugnG4uaHxD1z

#**Classificação de sequência com transformadores**

NoteBook Original: https://colab.research.google.com/drive/1l39vWjZ5jRUimSQDoUcuWGIoNjLjA2zu

Vamos, com uso da biblioteca Transformers na tarefa de classificação de sequência.

Usaremos dois modelos diferentes como meio de comparação: o BERT do Google e o RoBERTa do Facebook. Ambos têm a mesma arquitetura, mas tiveram abordagens de pré-treinamento diferentes.

**Instalando dependências necessárias**

Para importar os módulos TensorFlow, devemos ter certeza de que o TF2 está instalado no ambiente.
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
print(tf.__version__)

!pip install transformers

"""**Inicializando os modelos pré-treinados**

Inicializando os modelos com pesos pré-treinados. A lista de pesos pré-treinados está disponível na documentação oficial.
(https://huggingface.co/transformers/pretrained_models.html).
"""

from transformers import (TFBertForSequenceClassification,
                          BertTokenizer,
                          TFRobertaForSequenceClassification,
                          RobertaTokenizer)

#carregnado modelo BERT pre-treinado (serve para classificação de texto, tradução automática, geração de texto, etc)
# TFBertForSequenceClassification é a classe do transformes para tarefas de classificação de sequencia
bert_model = TFBertForSequenceClassification.from_pretrained("bert-base-cased")  # cased significa case sensitive

# divide o texto em tokens
# BertTokenizer classe que tokeniza
bert_tokenizer = BertTokenizer.from_pretrained("bert-base-cased")

roberta_model = TFRobertaForSequenceClassification.from_pretrained("roberta-base")
roberta_tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

#agora teremos o modelo pre-treinado

"""**### Tokenização**

BERT e RoBERTa são modelos de Transformer que possuem a mesma arquitetura. Como tal, eles aceitam apenas um certo tipo de entrada: vetores de números inteiros, cada valor representando um token. Cada sequência de texto deve primeiro ser convertida em uma lista de índices a serem alimentados no modelo. O tokenizer cuida disso para nós.

BERT e RoBERTa podem ter a mesma arquitetura, mas diferem na tokenização. BERT usa uma tokenização de subpalavra, enquanto RoBERTa usa a mesma tokenização que GPT-2: codificação de par de bytes em nível de byte. Essa tokenização em nível de byte é particularmente útil para lidar com idiomas que usam conjuntos de caracteres não latinos, símbolos especiais e caracteres raros. Ela lida com o texto em um nível mais granular, o que permite que o modelo lide com uma ampla variedade de caracteres e símbolos de maneira eficaz.

O GPT-2 usa essa técnica para tokenizar o texto em pares de bytes em nível de byte, o que o ajuda a entender e gerar texto em diferentes idiomas e contextos. Isso torna o modelo GPT-2 flexível e capaz de lidar com várias línguas e tipos de texto.
"""

#sequence ="Sou Antonio Leal. Aluno da UECE. Torcedor do LEAO"
sequence ="The identical rovers will act as robotic geologists , searching for evidence of past water "

# bert_tokenized_sequence é variavel
# bert_tokenizer é uma instância de BertTokenizer.from_pretrained("bert-base-cased")
# tokenize(sequence) é o metodo com argumento sequence
bert_tokenized_sequence = bert_tokenizer.tokenize(sequence)
roberta_tokenized_sequence = roberta_tokenizer.tokenize(sequence)

print("BERT:", bert_tokenized_sequence)
print("RoBERTa:", roberta_tokenized_sequence)

"""**Tokenizador BERT**

Aqui, o tokenizer BERT divide a string em várias substrings. Se as substrings estiverem em seu vocabulário, elas permanecerão como estão: este é o caso de `array`, `Antonio`. Porém, se uma string resultante não estiver em seu vocabulário, ela será dividida novamente até que cada string seja representada por seu vocabulário. Por exemplo, `Leal` é dividido várias vezes até que cada token seja representado no vocabulário BERT: ele é dividido em dois tokens. `UECE` é dividido em três tokens.

**Tokenizador RoBERTa**

Por outro lado, o tokenizer RoBERTa tem uma abordagem ligeiramente diferente. Aqui também, a string é dividida em múltiplas substrings, que são divididas em múltiplas substrings até que cada substring possa ser representada pelo vocabulário. No entanto, o tokenizer RoBERTa tem uma **abordagem em nível de byte**. Este tokenizer pode representar cada sequência como uma combinação de bytes.

**Obtendo resultados de última geração na classificação de sequências**

Para obter resultados de última geração nesta tarefa, ajustaremos nossos modelos em um determinado conjunto de dados. Ajustar um modelo significa que iremos treiná-lo ligeiramente em cima de um ponto de verificação já treinado. A taxa de aprendizagem será muito baixa, pois se for muito alta resultaria em um esquecimento catastrófico -> o modelo esqueceria o que aprendeu até agora semanticamente e sintaxicamente.

Seguiremos o procedimento detalhado abaixo:

1) Obtenha o conjunto de dados de `tensorflow_datasets`

2) Pré-processe este conjunto de dados para que possa ser usado pelo modelo

3) Configure um loop de treinamento usando a API fit de Keras; treinar o modelo nos dados de treinamento

4) Avalie o modelo com base nos dados de teste e compare com os resultados reais

** Obtendo o conjunto de dados**

Usaremos o conjunto de dados Microsoft Research Paraphrase Corpus (MRPC), que é um conjunto de dados de classificação de sequência. Obtemos os dados de treinamento e validação do pacote `tensorflow_datasets`. Esses valores estão no formato `tf.data.Dataset`, que é perfeito para nosso caso de uso.
"""

import tensorflow_datasets
data = tensorflow_datasets.load("glue/mrpc")

# o glue (General Language Understanding Evalution) é um modelo de conjunto de tarefas
# que vai avaliar desemepenho de modelos de linguagem
# mprc (Microsoft Research Paraphrase Corpus) conjunto de dados para classificação de texto,
# incluindo determinar se sentenças são paráfrases (iguais ou com mesmo sentido)

train_dataset = data["train"]  #variavel para guardare SIM ou NAO (1 ou 0) sobre os pares de sentenças
validation_dataset = data["validation"] #variavel com o conjunto de dados para validação do desempenho

"""Vamos gerar o valor do primeiro item do conjunto de dados para ter uma ideia dos dados com os quais estamos lidando."""

example = list(train_dataset.__iter__())[0]
print('',
    'id:      ', example['idx'],       '\n',
    'rótulo:    ', example['label'],     '\n',
    'sentença-1:', example['sentence1'], '\n',
    'sentenca-2:', example['sentence2'],
)

"""Os valores estão na forma de dicionários que são feitos da seguinte forma:

exemplo = {
     idx: número,
     rótulo: número,
     sentença-1: string,
     sentença-2: string
}

Os três campos que nos interessam são o `rótulo`, o `sentença1` e o `sentença2`. O rótulo é igual a `1` quando as duas sentenças são paráfrases uma da outra e é igual a `0` caso contrário.

Não podemos passar isso diretamente para os modelos, pois eles não conseguem interpretar o significado das strings (lista de caracteres), então veremos como converter esse exemplo em recursos que nossos modelos possam entender.

Primeiramente, devemos obter os IDs dos tokens que serão alimentados ao modelo a partir das duas sequências.

Os dois modelos (BERT, RoBERTa) possuem mecanismos de codificação diferentes em relação à codificação de sequências de pares, usando tokens `sep` e `cls` para especificar o final de uma sequência ou o final das duas sequências. Com `A` como a primeira sequência e `B` como a segunda, o BERT codifica o par de sequências da seguinte forma:

`[CLS] A [SET] B [SET]`

enquanto RoBERTa codifica o par de sequências de maneira diferente:

`[CLS] A [SET][SET] B [SET]`

Felizmente, nosso método encode pode lidar com isso sozinho. Aqui estão as duas frases com as quais estamos lidando atualmente:

"""

seq0 = example['sentence1'].numpy().decode('utf-8')
seq1 = example['sentence2'].numpy().decode('utf-8')

print("1a. sentença:", seq0)
print("2a. sentença:", seq1)

"""## Sequências de codificação

Para codificar a sequência para que seja compreensível pelo modelo, dois métodos diferentes podem ser úteis.

### codificar()

`encode` é um método de alto nível que retorna a sequência codificada com os tokens especiais e truncada para um comprimento máximo, se necessário. Aqui identificamos os tokens especiais SEP e CLS de RoBERTa e BERT.
"""

# seq0, seq1 (sequencias)
# add_special_tokens adiciona tokes adicionais (CLS e SEP) para separar as sequencias
# max_length é o tamanho máximo da sequencia
encoded_bert_sequence = bert_tokenizer.encode(seq0, seq1, add_special_tokens=True, max_length=128)
encoded_roberta_sequence = roberta_tokenizer.encode(seq0, seq1, add_special_tokens=True, max_length=128)

print("Separador tokenizador BERT, SEP CLS token id:   ", bert_tokenizer.sep_token_id, bert_tokenizer.cls_token_id)
print("Separador tokenizador RoBERTa, SEP CLS token id:", roberta_tokenizer.sep_token_id, roberta_tokenizer.cls_token_id)

# cria uma lista *(ber_special_tokens) com o SEP, CLS e ID
bert_special_tokens = [bert_tokenizer.sep_token_id, bert_tokenizer.cls_token_id]
roberta_special_tokens = [roberta_tokenizer.sep_token_id, roberta_tokenizer.cls_token_id]

def print_in_red(string):
    print("\033[91m" + str(string) + "\033[0m", end=' ')


# ver diferença deo word2vec e BERT NA TOKENIZAÇÃO

print("\nSEQUENCIA TOKENIZADA DE BERT")
output = [print_in_red(tok) if tok in bert_special_tokens else print(tok, end=' ') for tok in encoded_bert_sequence]

print("\nSEQUENCIA TOKENIZADA DE ROBERTa")
output = [print_in_red(tok) if tok in roberta_special_tokens else print(tok, end=' ') for tok in encoded_roberta_sequence]



"""#**### encode**

o `encode` retorna informações dos ids do tipo de token, bem como várias outras  carcteristiscas

Os IDs do tipo de token são usados por alguns modelos no caso de classificação de sequência. É uma máscara que indica ao modelo de qual sequência um token pertence.

Por exemplo, digamos que temos duas sequências A e B com tokens `[a0, a1, a2, a3]` e `[b0, b1, b2, b3, b4]` respectivamente.

O tokenizer BERT criaria uma única sequência a partir dessas duas listas de tokens que seria semelhante a esta:

<pre>
[tokens]         `[CLS] a0 a1 a2 a3 [SEP] b0 b1 b2 b3 b4 [SEP]`.
[token type ids] `  0    0  0  0  0   0    1  1  1  1  1   1`
</pre>

Graças aos IDs do tipo de token, o modelo sabe qual token pertence a qual sequência

Diretamente na biblioteca `Transformers` existe um método para converter um conjunto de dados em recursos. Este método usa `encode` e é chamado `glue_convert_examples_to_features`:

parametros:
bert_tokenizer e o q1ue é 128
"""

from transformers import glue_convert_examples_to_features

#adicionar mais caracteristicas ao modelo
# train_dataset é o conjunto de modelos
# bert_tokenizer instancia do tokenizer carregados
# 128 tamanho máximo da sequencia
# mrpc (Microsoft Research Paraphrase Corpus) conjunto de dados para classificação de texto,
bert_train_dataset = glue_convert_examples_to_features(train_dataset, bert_tokenizer, 128, 'mrpc')

# shuffle(100) um embraralhamento com número de 100 exemplos
# batch 32 = tamanho dos lotes
# repeat repete 2 vezes (epocas)
bert_train_dataset = bert_train_dataset.shuffle(100).batch(32).repeat(2)

# glue_convert_examples_to_features é a funcao que converte conjunto de dados glue como conjunto de validação mrpc
# validation_dataset (dados de validacao carregados antes)
# bert_tokenizer (instancia do tokenizer carregados antes)
# 128 ...
# mrpc ...
bert_validation_dataset = glue_convert_examples_to_features(validation_dataset, bert_tokenizer, 128, 'mrpc')
bert_validation_dataset = bert_validation_dataset.batch(64)

"""##Os dois conjuntos de dados BERT agora estão prontos para serem usados:

o conjunto de dados de treinamento é embaralhado e em lote, enquanto o conjunto de dados de validação é apenas em lote.

RoBERTa requer um pouco mais de trabalho, pois não usa token_type_ids, que precisamos remover. Usamos o método tf.data.Dataset.map() para isso.
"""

def token_type_ids_removal(example, label):
    del example["token_type_ids"]
    return example, label

roberta_train_dataset = glue_convert_examples_to_features(train_dataset, roberta_tokenizer, 128, 'mrpc')
#roberta_train_dataset = roberta_train_dataset.map(token_type_ids_removal)
roberta_train_dataset = roberta_train_dataset.shuffle(100).batch(32).repeat(2)

roberta_validation_dataset = glue_convert_examples_to_features(validation_dataset, roberta_tokenizer, 128, 'mrpc')
#roberta_validation_dataset = roberta_validation_dataset.map(token_type_ids_removal)
roberta_validation_dataset = roberta_validation_dataset.batch(64)

"""### Definindo os hiperparâmetros

Antes de ajustar o modelo, devemos definir alguns hiperparâmetros que serão utilizados durante o treinamento como o otimizador, a perda e a métrica de avaliação.

Como otimizador usaremos Adam, que foi o otimizador usado durante o pré-treinamento desses modelos. Como perda, usaremos a entropia cruzada categórica esparsa e a precisão categórica esparsa como métrica de avaliação.
"""

# otimizador Adam
# tf.keras.optimizers.Adam é a instancia do otimizador
# learning_rate taxa de aprendizagem
# epsilon valor adicionado para evitar divisões por zero
# clipnorm é é usado para limitar o valor da norma das atualizações dos gradientes.
# Isso ajuda a evitar atualizações muito grandes que podem tornar o treinamento instável.
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)

# tf.keras.losses.SparseCategoricalCrossentropy é a instancia de função de perda
# from_logits (modelo tá gerando pontuações de logit em vez de probabilidades diretamente)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# cria uma instância da métrica de acurácia categórica esparsa,
# que é adequada para problemas de classificação com rótulos esparsos (inteiros).
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

# compila o modelo passando os argumentos
bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])
roberta_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

"""### Treinando o modelo

tensorflow/keras com método fit do keras para ajustar o modelo com uma única linha de código
"""

print("Ajustando BERT no MRPC")

# bert_model.fit treina o modelo ajustando os pesos com base nos dadosd e treinamento
bert_history = bert_model.fit(bert_train_dataset, epochs=3, validation_data=bert_validation_dataset)

#print("\nFine-tuning RoBERTa on MRPC")
#roberta_history = roberta_model.fit(roberta_train_dataset, epochs=3, validation_data=roberta_validation_dataset)

"""### Avaliando um modelo"""

print("Avaliando modelo BERT")
bert_model.evaluate(bert_validation_dataset)

#print("Evaluating the RoBERTa model")
#roberta_model.evaluate(roberta_validation_dataset)